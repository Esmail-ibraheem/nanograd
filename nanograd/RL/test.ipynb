{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, parameters, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.state = {}\n",
    "\n",
    "        for p in self.parameters:\n",
    "            self.state[p] = {\n",
    "                'step': 0,\n",
    "                'exp_avg': 0,\n",
    "                'exp_avg_sq': 0,\n",
    "            }\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.parameters:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            grad = p.grad\n",
    "            state = self.state[p]\n",
    "\n",
    "            exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "            beta1, beta2 = self.betas\n",
    "\n",
    "            state['step'] += 1\n",
    "\n",
    "            exp_avg = beta1 * exp_avg + (1 - beta1) * grad\n",
    "            exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * (grad ** 2)\n",
    "            state['exp_avg'], state['exp_avg_sq'] = exp_avg, exp_avg_sq\n",
    "\n",
    "            denom = (exp_avg_sq.sqrt() / math.sqrt(state['step'])) + self.eps\n",
    "\n",
    "            step_size = self.lr / (1 - beta1 ** state['step'])\n",
    "\n",
    "            p.data -= step_size * (exp_avg / denom + self.weight_decay * p.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refactored Code Using Facade and Adapter Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**\n",
    "\n",
    "1. Parameter Class:\n",
    "\n",
    "- Represents a parameter with data and gradient attributes.\n",
    "- Includes a zero_grad method to reset the gradient.\n",
    "\n",
    "2. ParameterAdapter Class:\n",
    "\n",
    "- Adapts the Parameter class to the interface expected by the optimizer.\n",
    "- Provides properties to access and modify data and grad.\n",
    "\n",
    "3. Adam Class:\n",
    "\n",
    " - Implements the Adam optimizer with the necessary steps for parameter updates.\n",
    "\n",
    "4. OptimizerFacade Class:\n",
    "\n",
    "- Simplifies the interface for using the optimizer, providing zero_grad and step methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Parameter:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.grad = None\n",
    "\n",
    "class ParameterAdapter:\n",
    "    def __init__(self, parameter):\n",
    "        self.parameter = parameter\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self.parameter.data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, value):\n",
    "        self.parameter.data = value\n",
    "\n",
    "    @property\n",
    "    def grad(self):\n",
    "        return self.parameter.grad\n",
    "\n",
    "    @grad.setter\n",
    "    def grad(self, value):\n",
    "        self.parameter.grad = value\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.parameter.zero_grad()\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, parameters, lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        self.parameters = parameters\n",
    "        self.lr = lr\n",
    "        self.betas = betas\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.state = {}\n",
    "\n",
    "        for p in self.parameters:\n",
    "            self.state[p] = {\n",
    "                'step': 0,\n",
    "                'exp_avg': 0,\n",
    "                'exp_avg_sq': 0,\n",
    "            }\n",
    "\n",
    "    def step(self):\n",
    "        for p in self.parameters:\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "\n",
    "            grad = p.grad\n",
    "            state = self.state[p]\n",
    "\n",
    "            exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "            beta1, beta2 = self.betas\n",
    "\n",
    "            state['step'] += 1\n",
    "\n",
    "            exp_avg = beta1 * exp_avg + (1 - beta1) * grad\n",
    "            exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * (grad ** 2)\n",
    "            state['exp_avg'], state['exp_avg_sq'] = exp_avg, exp_avg_sq\n",
    "\n",
    "            denom = (exp_avg_sq ** 0.5) + self.eps\n",
    "\n",
    "            step_size = self.lr / (1 - beta1 ** state['step'])\n",
    "\n",
    "            p.data -= step_size * (exp_avg / denom + self.weight_decay * p.data)\n",
    "\n",
    "class OptimizerFacade:\n",
    "    def __init__(self, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.optimizer.parameters:\n",
    "            param.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Example usage\n",
    "params = [Parameter(0.5), Parameter(0.8)]\n",
    "adapted_params = [ParameterAdapter(p) for p in params]\n",
    "adam = Adam(adapted_params)\n",
    "optimizer = OptimizerFacade(adam)\n",
    "\n",
    "# Simulate gradient calculation\n",
    "for param in params:\n",
    "    param.grad = 0.1  # Example gradient\n",
    "\n",
    "# Optimization step\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n",
    "\n",
    "# Output the updated parameter values\n",
    "for param in params:\n",
    "    print(param.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "- This refactored code should now correctly update the parameter values using the Adam optimizer, while adhering to the Facade and Adapter design patterns. â€‹"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
